{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4412303c",
   "metadata": {},
   "source": [
    "CSCI 393: NLP\n",
    "Assignment 1: Tokenisation, lemmatisation &stemming, POS-tagging & NER\n",
    "Alibek Abilmazhit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6390e8c1",
   "metadata": {},
   "source": [
    "Question 1: Tokenisation\n",
    "\n",
    "Data:\n",
    "‚ÄúATG-CGA-TTT-AGC‚Äù\n",
    "‚ÄúThe quick brown fox jumps over the lazy dog.‚Äù\n",
    "‚ÄúJust landed in NYC!!! üòé‚úàÔ∏è #travel #blessed‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db096b6",
   "metadata": {},
   "source": [
    "(a) For each of the examples, decide what kind of tokenisation strategy would be most appropriate:\n",
    "1) ‚ÄúATG-CGA-TTT-AGC‚Äù: Rule-based splitting.\n",
    "2) ‚ÄúThe quick brown fox jumps over the lazy dog.‚Äù: Whitespace + punctuation-based tokenisation.\n",
    "3) ‚ÄúJust landed in NYC!!! üòé‚úàÔ∏è #travel #blessed‚Äù: Specialised tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2832f1",
   "metadata": {},
   "source": [
    "(b) Implement tokenisation in Python using at least two methods or libraries (e.g. NLTK‚Äôs word_tokenize, spaCy‚Äôs built-in tokeniser) and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a9d00df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1: NLTK word_tokenize\n",
      "\n",
      "ATG-CGA-TTT-AGC\n",
      "NLTK: ['ATG-CGA-TTT-AGC']\n",
      "The quick brown fox jumps over the lazy dog.\n",
      "NLTK: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n",
      "Just landed in NYC!!! üòé‚úàÔ∏è #travel #blessed\n",
      "NLTK: ['Just', 'landed', 'in', 'NYC', '!', '!', '!', 'üòé‚úàÔ∏è', '#', 'travel', '#', 'blessed']\n",
      "\n",
      "Method 2: TweetTokenizer\n",
      "\n",
      "TweetTokenizer: ['ATG-CGA-TTT-AGC']\n",
      "TweetTokenizer: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n",
      "TweetTokenizer: ['Just', 'landed', 'in', 'NYC', '!', '!', '!', 'üòé', '‚úà', 'Ô∏è', '#travel', '#blessed']\n",
      "\n",
      "Method 3: spaCy\n",
      "\n",
      "spaCy: ['ATG', '-', 'CGA', '-', 'TTT', '-', 'AGC']\n",
      "spaCy: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n",
      "spaCy: ['Just', 'landed', 'in', 'NYC', '!', '!', '!', 'üòé', '‚úà', 'Ô∏è', '#', 'travel', '#', 'blessed']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/alibekabilmazhit/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "import spacy\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "data = [\n",
    "    \"ATG-CGA-TTT-AGC\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Just landed in NYC!!! üòé‚úàÔ∏è #travel #blessed\"\n",
    "]\n",
    "\n",
    "print(\"Method 1: NLTK word_tokenize\\n\")\n",
    "for sentence in data:\n",
    "    print(sentence)\n",
    "    print(\"NLTK:\", word_tokenize(sentence))\n",
    "\n",
    "print(\"\\nMethod 2: TweetTokenizer\\n\")\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "for sentence in data:\n",
    "    print(\"TweetTokenizer:\", tweet_tokenizer.tokenize(sentence))\n",
    "\n",
    "print(\"\\nMethod 3: spaCy\\n\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "for text in data:\n",
    "    doc = nlp(text)\n",
    "    print(\"spaCy:\", [token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3111701",
   "metadata": {},
   "source": [
    "(c)  Discuss the pros and cons of each implemented tokeniser for each example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a0cf20",
   "metadata": {},
   "source": [
    "Question 2:  POS tagging\n",
    "Data:\n",
    "\"The quick brown fox jumps over the lazy dog.\"\n",
    "\"omg üòÇ can't believe @john_doe said that #shocked\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860c7552",
   "metadata": {},
   "source": [
    "(a) Use NLTK‚Äôs pos_tag on the tokenised text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47a4e548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/alibekabilmazhit/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/alibekabilmazhit/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumps over the lazy dog.\n",
      "POS Tags: [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n",
      "omg üòÇ can't believe @john_doe said that #shocked\n",
      "POS Tags: [('omg', 'NN'), ('üòÇ', 'NN'), ('ca', 'MD'), (\"n't\", 'RB'), ('believe', 'VB'), ('@', 'NNP'), ('john_doe', 'NN'), ('said', 'VBD'), ('that', 'IN'), ('#', '#'), ('shocked', 'VBD')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "data = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"omg üòÇ can't believe @john_doe said that #shocked\"\n",
    "]\n",
    "\n",
    "for sentence in data:\n",
    "    print(sentence)\n",
    "    tokens = word_tokenize(sentence)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    print(\"POS Tags:\", pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20ee57f",
   "metadata": {},
   "source": [
    "(b) Use spaCy‚Äôs doc[i].pos_ to get POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f520946c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy POS 1: [('The', 'DET'), ('quick', 'ADJ'), ('brown', 'ADJ'), ('fox', 'NOUN'), ('jumps', 'VERB'), ('over', 'ADP'), ('the', 'DET'), ('lazy', 'ADJ'), ('dog', 'NOUN'), ('.', 'PUNCT')]\n",
      "spaCy POS 2: [('omg', 'X'), ('üòÇ', 'PROPN'), ('ca', 'AUX'), (\"n't\", 'PART'), ('believe', 'VERB'), ('@john_doe', 'NUM'), ('said', 'VERB'), ('that', 'SCONJ'), ('#', 'PUNCT'), ('shocked', 'VERB')]\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"The quick brown fox jumps over the lazy dog.\")\n",
    "doc2 = nlp(\"omg üòÇ can't believe @john_doe said that #shocked\")\n",
    "\n",
    "print(\"spaCy POS 1:\", [(token.text, token.pos_) for token in doc1])\n",
    "print(\"spaCy POS 2:\", [(token.text, token.pos_) for token in doc2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a57e94",
   "metadata": {},
   "source": [
    "(c) Compare outputs ‚Äî are they identical? Where do they differ? Which one is better for the\n",
    "tweet?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
