{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4412303c",
   "metadata": {},
   "source": [
    "CSCI 393: NLP\n",
    "Assignment 1: Tokenisation, lemmatisation &stemming, POS-tagging & NER\n",
    "Alibek Abilmazhit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6390e8c1",
   "metadata": {},
   "source": [
    "Question 1: Tokenisation\n",
    "\n",
    "Data:\n",
    "‚ÄúATG-CGA-TTT-AGC‚Äù\n",
    "‚ÄúThe quick brown fox jumps over the lazy dog.‚Äù\n",
    "‚ÄúJust landed in NYC!!! üòé‚úàÔ∏è #travel #blessed‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db096b6",
   "metadata": {},
   "source": [
    "(a) For each of the examples, decide what kind of tokenisation strategy would be most appropriate:\n",
    "1) ‚ÄúATG-CGA-TTT-AGC‚Äù: Rule-based splitting.\n",
    "2) ‚ÄúThe quick brown fox jumps over the lazy dog.‚Äù: Whitespace + punctuation-based tokenisation.\n",
    "3) ‚ÄúJust landed in NYC!!! üòé‚úàÔ∏è #travel #blessed‚Äù: Specialised tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2832f1",
   "metadata": {},
   "source": [
    "(b) Implement tokenisation in Python using at least two methods or libraries (e.g. NLTK‚Äôs word_tokenize, spaCy‚Äôs built-in tokeniser) and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a9d00df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1: NLTK word_tokenize\n",
      "\n",
      "ATG-CGA-TTT-AGC\n",
      "NLTK: ['ATG-CGA-TTT-AGC']\n",
      "The quick brown fox jumps over the lazy dog.\n",
      "NLTK: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n",
      "Just landed in NYC!!! üòé‚úàÔ∏è #travel #blessed\n",
      "NLTK: ['Just', 'landed', 'in', 'NYC', '!', '!', '!', 'üòé‚úàÔ∏è', '#', 'travel', '#', 'blessed']\n",
      "\n",
      "Method 2: TweetTokenizer\n",
      "\n",
      "TweetTokenizer: ['ATG-CGA-TTT-AGC']\n",
      "TweetTokenizer: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n",
      "TweetTokenizer: ['Just', 'landed', 'in', 'NYC', '!', '!', '!', 'üòé', '‚úà', 'Ô∏è', '#travel', '#blessed']\n",
      "\n",
      "Method 3: spaCy\n",
      "\n",
      "spaCy: ['ATG', '-', 'CGA', '-', 'TTT', '-', 'AGC']\n",
      "spaCy: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n",
      "spaCy: ['Just', 'landed', 'in', 'NYC', '!', '!', '!', 'üòé', '‚úà', 'Ô∏è', '#', 'travel', '#', 'blessed']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/alibekabilmazhit/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "import spacy\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "data = [\n",
    "    \"ATG-CGA-TTT-AGC\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Just landed in NYC!!! üòé‚úàÔ∏è #travel #blessed\"\n",
    "]\n",
    "\n",
    "print(\"Method 1: NLTK word_tokenize\\n\")\n",
    "for sentence in data:\n",
    "    print(sentence)\n",
    "    print(\"NLTK:\", word_tokenize(sentence))\n",
    "\n",
    "print(\"\\nMethod 2: TweetTokenizer\\n\")\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "for sentence in data:\n",
    "    print(\"TweetTokenizer:\", tweet_tokenizer.tokenize(sentence))\n",
    "\n",
    "print(\"\\nMethod 3: spaCy\\n\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "for text in data:\n",
    "    doc = nlp(text)\n",
    "    print(\"spaCy:\", [token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3111701",
   "metadata": {},
   "source": [
    "(c)  Discuss the pros and cons of each implemented tokeniser for each example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a0cf20",
   "metadata": {},
   "source": [
    "Question 2:  POS tagging\n",
    "Data:\n",
    "\"The quick brown fox jumps over the lazy dog.\"\n",
    "\"omg üòÇ can't believe @john_doe said that #shocked\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860c7552",
   "metadata": {},
   "source": [
    "(a) Use NLTK‚Äôs pos_tag on the tokenised text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47a4e548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/alibekabilmazhit/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/alibekabilmazhit/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumps over the lazy dog.\n",
      "POS Tags: [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n",
      "omg üòÇ can't believe @john_doe said that #shocked\n",
      "POS Tags: [('omg', 'NN'), ('üòÇ', 'NN'), ('ca', 'MD'), (\"n't\", 'RB'), ('believe', 'VB'), ('@', 'NNP'), ('john_doe', 'NN'), ('said', 'VBD'), ('that', 'IN'), ('#', '#'), ('shocked', 'VBD')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "data = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"omg üòÇ can't believe @john_doe said that #shocked\"\n",
    "]\n",
    "\n",
    "for sentence in data:\n",
    "    print(sentence)\n",
    "    tokens = word_tokenize(sentence)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    print(\"POS Tags:\", pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20ee57f",
   "metadata": {},
   "source": [
    "(b) Use spaCy‚Äôs doc[i].pos_ to get POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f520946c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy POS 1: [('The', 'DET'), ('quick', 'ADJ'), ('brown', 'ADJ'), ('fox', 'NOUN'), ('jumps', 'VERB'), ('over', 'ADP'), ('the', 'DET'), ('lazy', 'ADJ'), ('dog', 'NOUN'), ('.', 'PUNCT')]\n",
      "spaCy POS 2: [('omg', 'X'), ('üòÇ', 'PROPN'), ('ca', 'AUX'), (\"n't\", 'PART'), ('believe', 'VERB'), ('@john_doe', 'NUM'), ('said', 'VERB'), ('that', 'SCONJ'), ('#', 'PUNCT'), ('shocked', 'VERB')]\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"The quick brown fox jumps over the lazy dog.\")\n",
    "doc2 = nlp(\"omg üòÇ can't believe @john_doe said that #shocked\")\n",
    "\n",
    "print(\"spaCy POS 1:\", [(token.text, token.pos_) for token in doc1])\n",
    "print(\"spaCy POS 2:\", [(token.text, token.pos_) for token in doc2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a57e94",
   "metadata": {},
   "source": [
    "(c) Compare outputs ‚Äî are they identical? Where do they differ? Which one is better for the\n",
    "tweet?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92bcf4f",
   "metadata": {},
   "source": [
    "Question 3: Stemming vs. Lemmatisation\n",
    "\n",
    "Example sentence:\n",
    "‚ÄúThe children are running and ate their meals quickly.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4c037b",
   "metadata": {},
   "source": [
    "(a) Use PorterStemmer or SnowballStemmer from NLTK to reduce words to their stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96b87ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stems: ['the', 'children', 'are', 'run', 'and', 'ate', 'their', 'meal', 'quickli', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"The children are running and ate their meals quickly.\"\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stems = [stemmer.stem(word) for word in word_tokenize(sentence)]\n",
    "print(\"Stems:\", stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca589ef",
   "metadata": {},
   "source": [
    "(b) Use token.lemma_ to get the lemma for each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5239db1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmas: ['the', 'child', 'be', 'run', 'and', 'eat', 'their', 'meal', 'quickly', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"The children are running and ate their meals quickly.\"\n",
    "\n",
    "lemmas = [word.lemma_ for word in nlp(sentence)]\n",
    "print(\"Lemmas:\", lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb6a4cd",
   "metadata": {},
   "source": [
    "(c) Compare the outputs - Why does stemming sometimes cut words awkwardly (e.g.,\n",
    "\"quickly\" ‚Üí \"quick\") while lemmatisation returns \"quickly\"? When is stemming still\n",
    "useful and when is lemmatisation preferable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a9a638",
   "metadata": {},
   "source": [
    "Question 4: Named Entity Recognition (NER)\n",
    "\n",
    "Data:\n",
    "‚ÄúElon Musk founded SpaceX in 2002 in California. In 2023, the company launched a mission\n",
    "that cost $500 million.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edcf3fe",
   "metadata": {},
   "source": [
    "(a) Use spaCy‚Äôs doc.ents to extract named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4305662f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Elon Musk, 2002, California, 2023, $500 million)\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Elon Musk founded SpaceX in 2002 in California. In 2023, the company launched a mission that cost $500 million.\")\n",
    "\n",
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c71d045",
   "metadata": {},
   "source": [
    "(b) Evaluate performance:\n",
    "‚óè Did spaCy miss any entity?\n",
    "‚óè Did it misclassify anything?\n",
    "‚óè Suggest one case where rule-based approaches (like regex) could work better, and\n",
    "one case where machine learning (NER) is superior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f953d9f",
   "metadata": {},
   "source": [
    "Question 5: Mini Project\n",
    "\n",
    "Take a piece of text of your choice in any language you like apart from English (but\n",
    "preferably one you understand). Run the full pipeline (make sure you choose appropriate\n",
    "tokenisers, POS taggers, lemmatisers and NERs for the source language):\n",
    "‚óè Choose an appropriate tokeniser\n",
    "‚óè POS tag\n",
    "‚óè Lemmatise\n",
    "‚óè Extract named entities\n",
    "\n",
    "Show all outputs (if using spaCy, use displacy for visualisation). Write a short report (1‚Äì2\n",
    "paragraphs) about:\n",
    "‚óè What kinds of words/entities were recognised well\n",
    "‚óè What errors or limitations you observed while tokenising, POS tagging and lemmatising"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
