{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4412303c",
   "metadata": {},
   "source": [
    "CSCI 393: NLP\n",
    "Assignment 1: Tokenisation, lemmatisation &stemming, POS-tagging & NER\n",
    "Alibek Abilmazhit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6390e8c1",
   "metadata": {},
   "source": [
    "Question 1: Tokenisation\n",
    "\n",
    "Data:\n",
    "â€œATG-CGA-TTT-AGCâ€\n",
    "â€œThe quick brown fox jumps over the lazy dog.â€\n",
    "â€œJust landed in NYC!!! ðŸ˜Žâœˆï¸ #travel #blessedâ€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db096b6",
   "metadata": {},
   "source": [
    "(a) For each of the examples, decide what kind of tokenisation strategy would be most appropriate:\n",
    "1) â€œATG-CGA-TTT-AGCâ€: Rule-based splitting.\n",
    "2) â€œThe quick brown fox jumps over the lazy dog.â€: Whitespace + punctuation-based tokenisation.\n",
    "3) â€œJust landed in NYC!!! ðŸ˜Žâœˆï¸ #travel #blessedâ€: Specialised tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2832f1",
   "metadata": {},
   "source": [
    "(b) Implement tokenisation in Python using at least two methods or libraries (e.g. NLTKâ€™s word_tokenize, spaCyâ€™s built-in tokeniser) and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a9d00df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1: NLTK word_tokenize\n",
      "\n",
      "ATG-CGA-TTT-AGC\n",
      "NLTK: ['ATG-CGA-TTT-AGC']\n",
      "The quick brown fox jumps over the lazy dog.\n",
      "NLTK: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n",
      "Just landed in NYC!!! ðŸ˜Žâœˆï¸ #travel #blessed\n",
      "NLTK: ['Just', 'landed', 'in', 'NYC', '!', '!', '!', 'ðŸ˜Žâœˆï¸', '#', 'travel', '#', 'blessed']\n",
      "\n",
      "Method 2: TweetTokenizer\n",
      "\n",
      "TweetTokenizer: ['ATG-CGA-TTT-AGC']\n",
      "TweetTokenizer: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n",
      "TweetTokenizer: ['Just', 'landed', 'in', 'NYC', '!', '!', '!', 'ðŸ˜Ž', 'âœˆ', 'ï¸', '#travel', '#blessed']\n",
      "\n",
      "Method 3: spaCy\n",
      "\n",
      "spaCy: ['ATG', '-', 'CGA', '-', 'TTT', '-', 'AGC']\n",
      "spaCy: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n",
      "spaCy: ['Just', 'landed', 'in', 'NYC', '!', '!', '!', 'ðŸ˜Ž', 'âœˆ', 'ï¸', '#', 'travel', '#', 'blessed']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/alibekabilmazhit/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "import spacy\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "data = [\n",
    "    \"ATG-CGA-TTT-AGC\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Just landed in NYC!!! ðŸ˜Žâœˆï¸ #travel #blessed\"\n",
    "]\n",
    "\n",
    "print(\"Method 1: NLTK word_tokenize\\n\")\n",
    "for sentence in data:\n",
    "    print(sentence)\n",
    "    print(\"NLTK:\", word_tokenize(sentence))\n",
    "\n",
    "print(\"\\nMethod 2: TweetTokenizer\\n\")\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "for sentence in data:\n",
    "    print(\"TweetTokenizer:\", tweet_tokenizer.tokenize(sentence))\n",
    "\n",
    "print(\"\\nMethod 3: spaCy\\n\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "for text in data:\n",
    "    doc = nlp(text)\n",
    "    print(\"spaCy:\", [token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3111701",
   "metadata": {},
   "source": [
    "(c)  Discuss the pros and cons of each implemented tokeniser for each example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a0cf20",
   "metadata": {},
   "source": [
    "Question 2:  POS tagging\n",
    "Data:\n",
    "\"The quick brown fox jumps over the lazy dog.\"\n",
    "\"omg ðŸ˜‚ can't believe @john_doe said that #shocked\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860c7552",
   "metadata": {},
   "source": [
    "(a) Use NLTKâ€™s pos_tag on the tokenised text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47a4e548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/alibekabilmazhit/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/alibekabilmazhit/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumps over the lazy dog.\n",
      "POS Tags: [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n",
      "omg ðŸ˜‚ can't believe @john_doe said that #shocked\n",
      "POS Tags: [('omg', 'NN'), ('ðŸ˜‚', 'NN'), ('ca', 'MD'), (\"n't\", 'RB'), ('believe', 'VB'), ('@', 'NNP'), ('john_doe', 'NN'), ('said', 'VBD'), ('that', 'IN'), ('#', '#'), ('shocked', 'VBD')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "data = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"omg ðŸ˜‚ can't believe @john_doe said that #shocked\"\n",
    "]\n",
    "\n",
    "for sentence in data:\n",
    "    print(sentence)\n",
    "    tokens = word_tokenize(sentence)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    print(\"POS Tags:\", pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20ee57f",
   "metadata": {},
   "source": [
    "(b) Use spaCyâ€™s doc[i].pos_ to get POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f520946c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy POS 1: [('The', 'DET'), ('quick', 'ADJ'), ('brown', 'ADJ'), ('fox', 'NOUN'), ('jumps', 'VERB'), ('over', 'ADP'), ('the', 'DET'), ('lazy', 'ADJ'), ('dog', 'NOUN'), ('.', 'PUNCT')]\n",
      "spaCy POS 2: [('omg', 'X'), ('ðŸ˜‚', 'PROPN'), ('ca', 'AUX'), (\"n't\", 'PART'), ('believe', 'VERB'), ('@john_doe', 'NUM'), ('said', 'VERB'), ('that', 'SCONJ'), ('#', 'PUNCT'), ('shocked', 'VERB')]\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"The quick brown fox jumps over the lazy dog.\")\n",
    "doc2 = nlp(\"omg ðŸ˜‚ can't believe @john_doe said that #shocked\")\n",
    "\n",
    "print(\"spaCy POS 1:\", [(token.text, token.pos_) for token in doc1])\n",
    "print(\"spaCy POS 2:\", [(token.text, token.pos_) for token in doc2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a57e94",
   "metadata": {},
   "source": [
    "(c) Compare outputs â€” are they identical? Where do they differ? Which one is better for the\n",
    "tweet?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92bcf4f",
   "metadata": {},
   "source": [
    "Question 3: Stemming vs. Lemmatisation\n",
    "\n",
    "Example sentence:\n",
    "â€œThe children are running and ate their meals quickly.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4c037b",
   "metadata": {},
   "source": [
    "(a) Use PorterStemmer or SnowballStemmer from NLTK to reduce words to their stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96b87ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stems: ['the', 'children', 'are', 'run', 'and', 'ate', 'their', 'meal', 'quickli', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"The children are running and ate their meals quickly.\"\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stems = [stemmer.stem(word) for word in word_tokenize(sentence)]\n",
    "print(\"Stems:\", stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca589ef",
   "metadata": {},
   "source": [
    "(b) Use token.lemma_ to get the lemma for each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5239db1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'lemma_'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[32m      3\u001b[39m sentence = \u001b[33m\"\u001b[39m\u001b[33mThe children are running and ate their meals quickly.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m lemmas = \u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlemma_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLemmas:\u001b[39m\u001b[33m\"\u001b[39m, lemmas)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[32m      3\u001b[39m sentence = \u001b[33m\"\u001b[39m\u001b[33mThe children are running and ate their meals quickly.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m lemmas = [\u001b[43mword\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlemma_\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_tokenize(sentence)]\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLemmas:\u001b[39m\u001b[33m\"\u001b[39m, lemmas)\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'lemma_'"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"The children are running and ate their meals quickly.\"\n",
    "\n",
    "lemmas = [word.lemma_ for word in nlp(sentence)]\n",
    "print(\"Lemmas:\", lemmas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
